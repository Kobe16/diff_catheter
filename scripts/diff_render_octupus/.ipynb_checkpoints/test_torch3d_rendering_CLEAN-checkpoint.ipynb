{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014a099d-03ee-4d3c-a92a-fd419cf02dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5052, 3])\n",
      "torch.Size([1, 5052, 3])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PerspectiveCameras,\n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex\n",
    ")\n",
    "\n",
    "import pytorch3d.utils as pytorch3d_utils\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the obj and ignore the textures and materials.\n",
    "verts, faces_idx, _ = load_obj(\"./blender_imgs/diff_render_1.obj\")\n",
    "\n",
    "# print(verts)\n",
    "\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.textures\n",
    "verts_rgb = torch.zeros_like(verts)[None]  # (1, V, 3)\n",
    "\n",
    "aaa = torch.zeros_like(verts).unsqueeze(0)  # (1, V, 3)\n",
    "\n",
    "\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures )\n",
    "\n",
    "print(aaa.shape)\n",
    "print(verts_rgb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515daeaf-3175-4643-bc14-eda6e5e49592",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'focal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# plt.axis(\"off\");\u001b[39;00m\n\u001b[1;32m     75\u001b[0m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfocal\u001b[49m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(principle)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(rot)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'focal' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Initialize a camera.\n",
    "# # With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction. \n",
    "# # So we move the camera by 180 in the azimuth direction so it is facing the front of the cow. \n",
    "\n",
    "# # R, T = look_at_view_transform(1.0, 0, 0) \n",
    "# cam_RT_H = torch.tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])\n",
    "# invert_y = torch.tensor([[-1., 0., 0., 0.], [0., -1., 0., 0.], [0., 0.,1., 0.], [0., 0., 0., 1.]])\n",
    "# cam_RT_H = torch.matmul(invert_y, cam_RT_H)\n",
    "# R = torch.unsqueeze(cam_RT_H[0:3,0:3], 0)\n",
    "\n",
    "# T = torch.tensor([[0., 0., 0.]])\n",
    "# cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "#### ============================================\n",
    "### From OPENCV Camera to Pytorch3D Camera\n",
    "#### ============================================\n",
    "# Given a projection matrix, obtain K, R, t\n",
    "cam_K = torch.tensor([[883.00220751, 0.0, 320.], [0.0, 883.0022075, 240.], [0., 0., 1.0]], device=device)\n",
    "\n",
    "cam_RT_H = torch.tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]], device=device)\n",
    "invert_y = torch.tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]], device=device)\n",
    "cam_RT_H = torch.matmul(invert_y, cam_RT_H)\n",
    "\n",
    "rot = (cam_RT_H[0:3,0:3]).unsqueeze(0)\n",
    "tvec =  (cam_RT_H[0:3, 3]).unsqueeze(0)\n",
    "\n",
    "\n",
    "# focal = torch.tensor((cam_K[0, 0],cam_K[1, 1]), dtype=torch.float32).unsqueeze(0)\n",
    "# principle = torch.tensor((cam_K[0, 2],cam_K[1, 2]), dtype=torch.float32).unsqueeze(0)\n",
    "cam_mat = cam_K.unsqueeze(0) \n",
    "\n",
    "# cameras = PerspectiveCameras(device=device, R=rot, T=trans, focal_length=focal, principal_point=principle, image_size=((480, 640),))\n",
    "\n",
    "image_size = torch.as_tensor([[480, 640]], device=device)\n",
    "cameras = pytorch3d_utils.cameras_from_opencv_projection(R=rot, tvec=tvec, camera_matrix=cam_mat, image_size=image_size)\n",
    "\n",
    "\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=(480,640), \n",
    "    blur_radius=0.0, \n",
    "    # faces_per_pixel=1, \n",
    ")\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of the cow is facing the \n",
    "# -z direction. \n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will \n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "images = renderer(mesh)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "# plt.axis(\"off\");\n",
    "\n",
    "images.shape\n",
    "print(focal)\n",
    "print(principle)\n",
    "print(rot)\n",
    "print(trans)\n",
    "\n",
    "# p_world = verts[-1].unsqueeze(0)\n",
    "\n",
    "# p_camera = cam_K @ (rot @ torch.transpose(p_world, 0, 1) - rot @ torch.transpose(trans, 0, 1))\n",
    "\n",
    "# p_world = verts\n",
    "\n",
    "\n",
    "# p_after_RT = (rot @ torch.transpose(p_world, 0, 1)).squeeze(0) - (rot @ torch.transpose(trans, 0, 1)).squeeze(0)\n",
    "# p_camera = cam_K @ p_after_RT\n",
    "\n",
    "# p_image = p_camera[0:2,:] / p_camera[2,:]\n",
    "\n",
    "# print(p_image)\n",
    "# print(p_camera)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
